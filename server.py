"""
ì˜í™” ì •ë³´ ë° ê°ì • ë¶„ì„ì„ ìœ„í•œ FastAPI ì„œë²„
"""

import sys
import torch
import re
import json
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from typing import Literal

from kobert_transformers import get_tokenizer
from transformers import BertForSequenceClassification
from chromadb import HttpClient

import torch.nn.functional as F

from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain.output_parsers import PydanticOutputParser

# ===== ëª¨ë¸ ë° ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì • =====
# KoBERT ëª¨ë¸ ë¡œë“œ
from dotenv import load_dotenv
import os
from openai import OpenAI
import uuid
from langchain_core.runnables import RunnableSequence

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

model_path = os.getenv("MODEL_PATH")
kobert_tokenizer = get_tokenizer()
kobert_model = BertForSequenceClassification.from_pretrained(model_path, local_files_only=True)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
kobert_model.to(device)

openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
chroma_client = HttpClient(host="chromadb", port=8000)

QUESTION_COLLECTION_NAME = "question-collection"
question_collection = chroma_client.get_or_create_collection(name=QUESTION_COLLECTION_NAME)


movie_collection = chroma_client.get_or_create_collection(name="movie-collection")

# ì˜í™” ì œëª© ìºì‹œ
movie_titles = set()
for metadata in movie_collection.get()["metadatas"]:
    title = metadata.get("title")
    if title:
        movie_titles.add(title)

# ===== FastAPI ì•± ì„¤ì • =====
app = FastAPI()

# CORS ë¯¸ë“¤ì›¨ì–´ ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://ownarea.synology.me"],
    allow_credentials=False,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ===== ë°ì´í„° ëª¨ë¸ =====
class MessageType(BaseModel):
    type: Literal["ê°ì •", "ëŒ€í™”", "ì§ˆì˜", "ëª©ë¡"] = Field(description="ë©”ì‹œì§€ì˜ ìœ í˜•")
    confidence: float = Field(description="ë¶„ë¥˜ ì‹ ë¢°ë„")

class ChatRequest(BaseModel):
    message: str

# ===== ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ =====
def normalize_text(text: str) -> str:
    """
    í…ìŠ¤íŠ¸ë¥¼ ì •ê·œí™”í•˜ëŠ” í•¨ìˆ˜
    - ê´„í˜¸ ì•ˆì˜ ë‚´ìš© ì œê±°
    - ê³µë°± ì œê±°
    - ì†Œë¬¸ì ë³€í™˜
    """
    text = re.sub(r"\(.*?\)", "", text) # ê´„í˜¸ ì•ˆì˜ ë‚´ìš© ì œê±°
    text = re.sub(r"\s+", "", text) # ê³µë°± ì œê±°
    return text.lower() # ì†Œë¬¸ì ë³€í™˜

def extract_title_from_query(query: str) -> str | None: # ì˜í™” ì œëª© ì¶”ì¶œ(íŠœí”Œ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜)
    """
    ì¿¼ë¦¬ì—ì„œ ì˜í™” ì œëª©ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
    - ì •ê·œí™”ëœ ì¿¼ë¦¬ì™€ í‚¤ì›Œë“œë¥¼ ë¹„êµí•˜ì—¬ ë§¤ì¹­ë˜ëŠ” ì˜í™” ì œëª© ë°˜í™˜
    """
    normalized_query = normalize_text(query) # ì •ê·œí™”ëœ ì¿¼ë¦¬ ìƒì„±
    result = movie_collection.get(include=["metadatas", "documents"]) # ì˜í™” ì •ë³´ ì¡°íšŒ
    
    for doc, meta in zip(result["documents"], result["metadatas"]): # ì˜í™” ì •ë³´ì™€ ë©”íƒ€ë°ì´í„° ë™ì‹œ ìˆœíšŒ
        raw_keywords = meta.get("keywords", []) # í‚¤ì›Œë“œ ì¡°íšŒ
        if isinstance(raw_keywords, str): # í‚¤ì›Œë“œê°€ ë¬¸ìì—´ì¸ ê²½ìš°
            try:
                keyword_list = json.loads(raw_keywords)
            except json.JSONDecodeError:
                keyword_list = []
        elif isinstance(raw_keywords, list):
            keyword_list = raw_keywords
        else:
            keyword_list = []
            
        keyword_list = [normalize_text(k) for k in keyword_list if isinstance(k, str)]
        if any(k in normalized_query for k in keyword_list):
            return meta.get("title")
    return None

def predict_sentiment(sentence: str) -> str:
    """
    KoBERTë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì¥ì˜ ê°ì •ì„ ë¶„ì„í•˜ëŠ” í•¨ìˆ˜
    - ê¸ì •/ë¶€ì •/ì¤‘ë¦½ ë¶„ë¥˜ ê²°ê³¼ ë°˜í™˜
    """
    inputs = kobert_tokenizer(sentence, return_tensors="pt", padding=True, truncation=True, max_length=128)
    inputs = {k: v.to(device) for k, v in inputs.items()}
    kobert_model.eval()

    with torch.no_grad():
        outputs = kobert_model(**inputs)
        logits = outputs.logits
        probs = F.softmax(logits, dim=1)  # í™•ë¥ ê°’ ê³„ì‚°
        positive_prob = probs[0][1].item()
        negative_prob = probs[0][0].item()

    print(f"ê¸ì •: {positive_prob}, ë¶€ì •: {negative_prob}, ê°€ì¤‘ì¹˜: {abs(positive_prob - negative_prob)}")
    # ì¤‘ë¦½ ê¸°ì¤€ê°’ ì„¤ì • (ì˜ˆ: í™•ì‹ ì´ 0.6 ë¯¸ë§Œì´ë©´ ì¤‘ë¦½ ì²˜ë¦¬)
    if abs(positive_prob - negative_prob) < 0.8:
        return "ì¤‘ë¦½"
    elif positive_prob > negative_prob:
        return "ê¸ì •"
    else:
        return "ë¶€ì •"

def classify_message(message: str) -> MessageType:
    """
    LangChainì„ ì‚¬ìš©í•˜ì—¬ ë©”ì‹œì§€ ìœ í˜•ì„ ë¶„ë¥˜í•˜ëŠ” í•¨ìˆ˜
    """
    llm = ChatOpenAI(temperature=0, openai_api_key=openai_client.api_key)
    
    template = """
    ë‹¤ìŒ ë©”ì‹œì§€ë¥¼ ë³´ê³  ì•„ë˜ 4ê°€ì§€ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”:

    - ê°ì •: ì˜í™”ì— ê´€ë ¨ëœ í‰ê°€ê°€ í¬í•¨ì´ ë˜ì–´ì•¼ í•˜ë©° í™”ë‚¨, ê¸°ì¨, ì§œì¦, ìš°ìš¸, ì„¤ë ˜ ë“± **ê°ì • ìƒíƒœ**ë¥¼ í‘œí˜„í•œ ë¬¸ì¥ (ì˜ˆ: "ì´ ì˜í™” ë‚´ìš©ì´ ì§œì¦ë‚˜", "ì£¼ì¸ê³µ ì—°ê¸°ê°€ ë„˜ì‚¬ë²½ì´ì•¼", "ìŠ¤í† ë¦¬ê°€ ì¢€ ì–´ì •ì©¡í•˜ë„¤")
    - ëŒ€í™”: ì¸ì‚¬, ì¼ìƒ ëŒ€í™”, ì¡ë‹´ ë“± (ì˜ˆ: "ì•ˆë…•?", "ë°¥ ë¨¹ì—ˆì–´?", "ì˜ ì§€ë‚´?")
    - ì§ˆì˜: **íŠ¹ì • ì˜í™” ì œëª©ì„ í¬í•¨í•˜ë©°**, í•´ë‹¹ ì˜í™”ì— ëŒ€í•´ ì§ˆë¬¸í•˜ëŠ” ë¬¸ì¥ (ì˜ˆ: "ê¸°ìƒì¶© ì¤„ê±°ë¦¬ ì•Œë ¤ì¤˜", "ì¥¬ë§Œì§€ í‰ì ì€?")
    - ëª©ë¡: **íŠ¹ì • ì˜í™” ì œëª© ì—†ì´**, ì˜í™” ì¶”ì²œì´ë‚˜ ëª©ë¡ì„ ìš”ì²­í•˜ëŠ” ë¬¸ì¥ (ì˜ˆ: "ìµœì‹  ì˜í™” ë­ ìˆì–´?", "ìš”ì¦˜ ë³¼ë§Œí•œ ì˜í™” ì¶”ì²œí•´ì¤˜")

    ë©”ì‹œì§€: {message}

    {format_instructions}
    """
    
    parser = PydanticOutputParser(pydantic_object=MessageType)
    prompt = PromptTemplate(
        template=template,
        input_variables=["message"],
        partial_variables={"format_instructions": parser.get_format_instructions()}
    )
    
    chain = prompt | llm
    result = chain.invoke({"message": message})
    
    return parser.parse(result.content)

# ===== ì˜í™” ì •ë³´ ê´€ë ¨ í•¨ìˆ˜ =====
def list_movie_titles_from_rag() -> str:
    """
    RAGì— ë“±ë¡ëœ ëª¨ë“  ì˜í™” ì œëª© ëª©ë¡ì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
    """
    result = movie_collection.get(include=["metadatas"])
    titles = [meta.get("title", "(ì œëª© ì—†ìŒ)") for meta in result["metadatas"] if meta.get("title")]
    
    if not titles:
        return "ë“±ë¡ëœ ì˜í™”ê°€ ì—†ìŠµë‹ˆë‹¤."
        
    response = "ê²€ìƒ‰ëœ ì˜í™” ëª©ë¡ì…ë‹ˆë‹¤:\n\n"
    for idx, title in enumerate(sorted(titles), 1):
        response += f"{idx}. {title}\n"
    return response.strip()

def get_movie_detail_by_title(title: str) -> str:
    """
    ì˜í™” ì œëª©ìœ¼ë¡œ ìƒì„¸ ì •ë³´ë¥¼ ì¡°íšŒí•˜ëŠ” í•¨ìˆ˜
    """
    result = movie_collection.get(include=["documents", "metadatas"])
    for doc, meta in zip(result["documents"], result["metadatas"]):
        if meta.get("title") == title:
            return f"ì˜í™” ì œëª©: {title}\n\nê°œë´‰ì¼: {meta.get('release_date')}\n\nì¥ë¥´: {meta.get('genre')}\n\nêµ­ê°€: {meta.get('country')}\n\nëŸ°ë‹íƒ€ì„: {meta.get('running_time')}ë¶„\n\nì¤„ê±°ë¦¬:{doc}"
    return "ë“±ë¡ëœ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."

def get_movie_detail_or_fallback(query: str) -> tuple[str | None, str]:
    """
    ì˜í™” ì •ë³´ë¥¼ ì¡°íšŒí•˜ëŠ” í•¨ìˆ˜
    - í‚¤ì›Œë“œ ë§¤ì¹­ì„ ì‹œë„í•˜ê³ , ì‹¤íŒ¨ ì‹œ fallback ë°©ì‹ìœ¼ë¡œ ê²€ìƒ‰
    """
    # í‚¤ì›Œë“œ ë§¤ì¹­ ì‹œë„
    extracted_title = extract_title_from_query(query)
    if extracted_title:
        return get_movie_detail_by_title(extracted_title), "RAG í‚¤ì›Œë“œ ë§¤ì¹­"
    
    # Fallback: í† í° ê¸°ë°˜ ê²€ìƒ‰
    tokens = [t.lower() for t in re.findall(r'[ã„±-ã…£ê°€-í£a-zA-Z0-9]+', query)]
    result = movie_collection.get(include=["metadatas", "documents"])
    match_scores = []
    
    for meta, doc in zip(result["metadatas"], result["documents"]):
        raw_keywords = meta.get("keywords", [])
        if isinstance(raw_keywords, str):
            try:
                keyword_list = json.loads(raw_keywords)
            except:
                keyword_list = []
        elif isinstance(raw_keywords, list):
            keyword_list = raw_keywords
        else:
            keyword_list = []
            
        keyword_list = [k.strip().lower() for k in keyword_list]
        score = sum(1 for token in tokens if token in keyword_list)
        if score > 0:
            match_scores.append((score, meta.get("title"), doc))
    
    if match_scores:
        match_scores.sort(reverse=True)
        _, best_title, best_doc = match_scores[0]
        return f"ğŸ“½ï¸ ì˜í™” ì œëª©: {best_title}\n\n{best_doc}", "RAG í‚¤ì›Œë“œ fallback"
    
    return None, "RAG ì—†ìŒ"

def get_movie_info_from_gpt(query: str) -> str:
    """
    GPTë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜í™” ì •ë³´ë¥¼ ì–»ëŠ” í•¨ìˆ˜
    """
    llm = ChatOpenAI(temperature=0.7)
    
    template = """
    ë‹¹ì‹ ì€ ì˜í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ì˜í™” ê´€ë ¨ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.
    **ë‹¹ì‹ ì€ ì˜í™” ê´€ë ¨ëœ ì§ˆë¬¸ ì™¸ì—ëŠ” ë‹µë³€í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ ê·œì¹™ì€ ì ˆëŒ€ ì–´ê¸¸ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.**
    ê°€ëŠ¥í•œ í•œ 200ì ì´ë‚´ë¡œ ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.
    
    ì§ˆë¬¸: {query}
    
    ë‹µë³€:
    """
    
    prompt = PromptTemplate(
        template=template,
        input_variables=["query"]
    )
    
    chain = LLMChain(llm=llm, prompt=prompt)
    result = chain.invoke({"query": query})
    
    return result["text"]

def save_question_answer_pair(question: str, answer: str):
    """
    ì§ˆë¬¸-ì‘ë‹µ ìŒì„ ChromaDBì— ì €ì¥
    """
    try:
        response = openai_client.embeddings.create(
            model="text-embedding-ada-002",
            input=question
        )
        embedding = response.data[0].embedding

        question_collection.add(
            ids=[str(uuid.uuid4())],
            documents=[answer],
            embeddings=[embedding],
            metadatas={"question": question}
        )
    except Exception as e:
        print(f"ì§ˆë¬¸-ì‘ë‹µ ì €ì¥ ì‹¤íŒ¨: {e}")

def find_similar_question_answer(query: str, threshold: float = 0.80) -> str | None:
    try:
        normalized_query = normalize_text(query)
        
        response = openai_client.embeddings.create(
            model="text-embedding-ada-002",
            input=normalized_query
        )
        embedding = response.data[0].embedding

        result = question_collection.query(
            query_embeddings=[embedding],
            n_results=1,
            include=["documents", "metadatas", "distances"]
        )

        documents = result.get("documents", [])
        distances = result.get("distances", [])

        if not documents or not distances or not documents[0] or not distances[0]:
            return None  # ìœ ì‚¬ ë¬¸ì„œ ì—†ìŒ

        score = distances[0][0]
        if score < (1.0 - threshold):
            return documents[0][0]
        return None

    except Exception as e:
        print(f"ì¤‘ë³µ ì§ˆë¬¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return None

# ===== API ì—”ë“œí¬ì¸íŠ¸ =====
@app.post("/chat")
async def chat_endpoint(request: ChatRequest):
    user_message = request.message.strip()
    
    # ë©”ì‹œì§€ ìœ í˜• ë¶„ë¥˜
    message_type = classify_message(user_message)
    
    if message_type.type == "ê°ì •":
        sentiment = predict_sentiment(user_message)
        return {"reply": f"ê°ì • ë¶„ì„ ê²°ê³¼: {sentiment}", "mode": "KoBERT í•™ìŠµëª¨ë¸(nsmc=ì˜í™”ë¦¬ë·° ë°ì´í„°) ê°ì • ë¶„ì„"}
    
    elif message_type.type == "ëª©ë¡":
        return {"reply": list_movie_titles_from_rag(), "mode": "RAGì— ìˆëŠ” ëª©ë¡"}
    
    elif message_type.type == "ì§ˆì˜":
        # 1. ìºì‹œ ë¨¼ì € í™•ì¸
        cached = find_similar_question_answer(user_message)
        if cached:
            return {"reply": cached, "mode": "ì¤‘ë³µ ì§ˆë¬¸ ì‘ë‹µ ìºì‹œ"}
        
        # 2. RAG ì¡°íšŒ ì‹œë„
        reply, mode = get_movie_detail_or_fallback(user_message)
        if reply:
            save_question_answer_pair(user_message, reply)
            return {"reply": reply, "mode": mode}
        
        # ì •ë³´ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš° GPTì—ê²Œ ì§ˆë¬¸
        gpt_response = get_movie_info_from_gpt(user_message)
        save_question_answer_pair(user_message, gpt_response)
        return {"reply": gpt_response, "mode": "GPT ì‘ë‹µ"}
    
    else:  # ëŒ€í™”
        return {"reply": "ì˜í™”ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹œë‹¤ë©´ ë¬¼ì–´ë³´ì„¸ìš”!", "mode": "ëŒ€í™”"}